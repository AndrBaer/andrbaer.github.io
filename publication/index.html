<!DOCTYPE html><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.5.0 for Hugo" />
  

  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Andreas Bär" />

  
  
  
    
  
  <meta name="description" content="Hey there, welcome to my personal website! :)" />

  
  <link rel="alternate" hreflang="en-us" href="https://andrbaer.github.io/publication/" />

  
  
  
    <meta name="theme-color" content="#1565c0" />
  

  
  

  

  <link rel="stylesheet" href="../css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  <link rel="stylesheet" href="../css/wowchemy.1052fab8b7700a3dc49ee23683097d66.css" />

  



  


  


  




  
  
  

  
    <link rel="alternate" href="../publication/index.xml" type="application/rss+xml" title="Andreas Bär" />
  

  
    <link rel="manifest" href="../manifest.webmanifest" />
  

  <link rel="icon" type="image/png" href="../media/icon_hu6f3f00bf32b63b175660be6614499f28_43426_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="../media/icon_hu6f3f00bf32b63b175660be6614499f28_43426_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://andrbaer.github.io/publication/" />

  
  
  
  
  
  
  
  
    
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="Andreas Bär" />
  <meta property="og:url" content="https://andrbaer.github.io/publication/" />
  <meta property="og:title" content="Publications | Andreas Bär" />
  <meta property="og:description" content="Hey there, welcome to my personal website! :)" /><meta property="og:image" content="https://andrbaer.github.io/media/icon_hu6f3f00bf32b63b175660be6614499f28_43426_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="https://andrbaer.github.io/media/icon_hu6f3f00bf32b63b175660be6614499f28_43426_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta property="og:updated_time" content="2023-06-16T00:00:00&#43;00:00" />
    
  

  



  

  





  <title>Publications | Andreas Bär</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="3a079e7dad19be978a318345a7749d34" >

  
  
  
  
  
  
  
  
  
  <script src="../js/wowchemy-init.min.2ed908358299dd7ab553faae685c746c.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<header class="header--fixed">
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="../">Andreas Bär</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="../">Andreas Bär</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="../#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="../#accomplishments"><span>Awards</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="../#experience"><span>Experience</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="../#projects"><span>Projects</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="../#publications"><span>Publications</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    















  

  
  
  
    
  
<div class="universal-wrapper pt-3">
  <h1>Publications</h1>

  

  
</div>



<div class="universal-wrapper">
  <div class="row">
    <div class="col-lg-12">

      

      
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      

      <div class="form-row mb-4">
        <div class="col-auto">
          <input type="search" class="filter-search form-control form-control-sm" placeholder="Search..." autocapitalize="off"
          autocomplete="off" autocorrect="off" role="textbox" spellcheck="false">
        </div>
        <div class="col-auto">
          <select class="pub-filters pubtype-select form-control form-control-sm" data-filter-group="pubtype">
            <option value="*">Type</option>
            
            
            <option value=".pubtype-1">
              Conference paper
            </option>
            
            <option value=".pubtype-2">
              Journal article
            </option>
            
            <option value=".pubtype-3">
              Preprint
            </option>
            
            <option value=".pubtype-6">
              Book section
            </option>
            
          </select>
        </div>
        <div class="col-auto">
          <select class="pub-filters form-control form-control-sm" data-filter-group="year">
            <option value="*">Date</option>
            
            
            
            <option value=".year-2023">
              2023
            </option>
            
            <option value=".year-2022">
              2022
            </option>
            
            <option value=".year-2021">
              2021
            </option>
            
            <option value=".year-2020">
              2020
            </option>
            
            <option value=".year-2019">
              2019
            </option>
            
            
          </select>
        </div>
      </div>

      <div id="container-publications">
        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2023">
          





  








  






<div class="col-lg-12 mb-5 view-showcase">
  <div class="row align-items-center">
    <div class="col-12 col-md-6">
      <div class="section-subheading article-title mb-0 mt-0">Improvements to Image Reconstruction-Based Performance Prediction for Semantic Segmentation in Highly Automated Driving</div>

      
      <div class="article-style">
        The performance of deep neural networks is typically measured with ground truth data which is expensive and not available during operation. At the same time, safety-critical applications, such as highly automated driving, require an awareness of the current performance, especially during operation with distorted inputs. Recently, performance prediction for semantic segmentation by an image reconstruction decoder was proposed. In this work, we investigate three approaches to improve its predictive power: Parameter initialization, parameter sharing, and inter-decoder lateral connections. Our best setup establishes a new state of the art in performance prediction with image-only inputs on Cityscapes and KITTI and even excels a method exploiting both point cloud and image inputs on Cityscapes. Further, our investigations reveal that the best Pearson correlation between the segmentation quality and the reconstruction quality does not always lead to the best predictive power. Code is available at <a href="https://github.com/ifnspaml/PerfPredRecV2" target="_blank" rel="noopener">https://github.com/ifnspaml/PerfPredRecV2</a>.
      </div>
      

      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="https://openaccess.thecvf.com/content/CVPR2023W/WAD/papers/Bar_Improvements_to_Image_Reconstruction-Based_Performance_Prediction_for_Semantic_Segmentation_in_CVPRW_2023_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal"
        data-filename="/publication/baer2023/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header" href="https://github.com/ifnspaml/PerfPredRecV2" target="_blank" rel="noopener">
  Code
</a>














      </div>

    </div>
    <div class="col-12 col-md-6 order-first ">
      
      
    </div>
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2023">
          





  








  




  



<div class="col-lg-12 mb-5 view-showcase">
  <div class="row align-items-center">
    <div class="col-12 col-md-6">
      <div class="section-subheading article-title mb-0 mt-0">A Novel Benchmark for Refinement of Noisy Localization Labels in Autolabeled Datasets for Object Detection</div>

      
      <div class="article-style">
        Autolabeling approaches are attractive w.r.t. time and cost as they allow fast annotation without human intervention. However, can we really trust the label quality of autolabeling? And further, which potential consequences arise from resulting label noise? In this work, we address these questions for localization, a subtask of object detection, by investigating the effects on a state-of-the-art deep neural network (DNN) for object detection and the widely used Pascal VOC 2012 dataset. Our contributions are threefold: First, we propose a method to inject noise into localization labels, enabling us to simulate localization label errors of autolabeling methods. Afterwards, we train a state-of-the-art object detection DNN with these noisy labels. Second, we propose a refinement network which takes a noisy localization label and its respective image as input and performs a localization refinement. Third, we again train a state-of-the-art object detection DNN, however, this time with refined localization labels. Our insights are: Training a state-of-the-art DNN for object detection on noisy localization labels leads to a severe performance drop. Our proposed localization label refinement network is able to refine the noisy localization labels. We are able to retain the performance to some extent by retraining the state-of-the-art DNN for object detection on the refined localization labels. Our study motivates a new challenging task &lsquo;refinement of noisy localization labels&rsquo; and sets a first benchmark for Pascal VOC 2012. Code is available at <a href="https://github.com/ifnspaml/LocalizationLabelNoise" target="_blank" rel="noopener">https://github.com/ifnspaml/LocalizationLabelNoise</a>.
      </div>
      

      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="https://openaccess.thecvf.com/content/CVPR2023W/SAIAD/papers/Bar_A_Novel_Benchmark_for_Localization_Label_Errors_and_Their_Refinement_CVPRW_2023_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal"
        data-filename="/publication/baer2023a/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header" href="https://github.com/ifnspaml/LocalizationLabelNoise" target="_blank" rel="noopener">
  Code
</a>














      </div>

    </div>
    <div class="col-12 col-md-6 order-first order-md-2">
      
      
    </div>
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022">
          





  








  






<div class="col-lg-12 mb-5 view-showcase">
  <div class="row align-items-center">
    <div class="col-12 col-md-6">
      <div class="section-subheading article-title mb-0 mt-0">Detecting Adversarial Perturbations in Multi-Task Perception</div>

      
      <div class="article-style">
        While deep neural networks (DNNs) achieve impressive performance on environment perception tasks, their sensitivity to adversarial perturbations limits their use in practical applications. In this paper, we (i) propose a novel adversarial perturbation detection scheme based on multi-task perception of complex vision tasks (i.e., depth estimation and semantic segmentation). Specifically, adversarial perturbations are detected by inconsistencies between extracted edges of the input image, the depth output, and the segmentation output. To further improve this technique, we (ii) develop a novel edge consistency loss between all three modalities, thereby improving their initial consistency which in turn supports our detection scheme. We verify our detection scheme&rsquo;s effectiveness by employing various known attacks and image noises. In addition, we (iii) develop a multi-task adversarial attack, aiming at fooling both tasks as well as our detection scheme. Experimental evaluation on the Cityscapes and KITTI datasets shows that under an assumption of a 5% false positive rate up to 100% of images are correctly detected as adversarially perturbed, depending on the strength of the perturbation. Code is available at <a href="https://github.com/ifnspaml/AdvAttackDet" target="_blank" rel="noopener">this https URL</a>. A short video at <a href="https://www.youtube.com/watch?v=KKa6gOyWmH4&amp;feature=youtu.be" target="_blank" rel="noopener">this https URL</a> provides qualitative results.
      </div>
      

      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="https://arxiv.org/pdf/2203.01177.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal"
        data-filename="/publication/klingner2022/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header" href="https://github.com/ifnspaml/AdvAttackDet" target="_blank" rel="noopener">
  Code
</a>














      </div>

    </div>
    <div class="col-12 col-md-6 order-first ">
      
      
    </div>
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022">
          





  








  




  



<div class="col-lg-12 mb-5 view-showcase">
  <div class="row align-items-center">
    <div class="col-12 col-md-6">
      <div class="section-subheading article-title mb-0 mt-0">Performance Prediction for Semantic Segmentation and by a Self-Supervised Image Reconstruction Decoder</div>

      
      <div class="article-style">
        In supervised learning, a deep neural network’s performance is measured using ground truth data. In semantic segmentation, ground truth data is sparse, requires an expensive annotation process, and, most importantly, it is not available during online operation. To tackle this problem, recent works propose various forms of performance prediction. However, they either rely on inference data histograms, additional sensors, or additional training data. In this paper, we propose a novel per-image performance prediction for semantic segmentation, with (i) no need for additional sensors (sensor efficiency), (ii) no need for additional training data (data efficiency), and (iii) no need for a dedicated retraining of the semantic segmentation (training efficiency). Specifically, we extend an already trained semantic segmentation network having fixed parameters with an image reconstruction decoder. After training and a subsequent regression, the image reconstruction quality is evaluated to predict the semantic segmentation performance. We demonstrate our method’s effectiveness with a new state-ofthe-art benchmark both on KITTI and Cityscapes for imageonly input methods, on Cityscapes even excelling a LiDARsupported benchmark.
      </div>
      

      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="https://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Bar_Performance_Prediction_for_Semantic_Segmentation_by_a_Self-Supervised_Image_Reconstruction_CVPRW_2022_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal"
        data-filename="/publication/baer2022/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header" href="https://github.com/ifnspaml/PerfPredRec" target="_blank" rel="noopener">
  Code
</a>














      </div>

    </div>
    <div class="col-12 col-md-6 order-first order-md-2">
      
      
    </div>
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022">
          





  








  






<div class="col-lg-12 mb-5 view-showcase">
  <div class="row align-items-center">
    <div class="col-12 col-md-6">
      <div class="section-subheading article-title mb-0 mt-0">Adaptive Bitrate Quantization Scheme Without Codebook for Learned Image Compression</div>

      
      <div class="article-style">
        We propose a generic approach to quantization without codebook in learned image compression called onehot max (OHM, Ω) quantization. It reorganizes the feature space resulting in an additional dimension, along which vector quantization yields one-hot vectors by comparing activations. Furthermore, we show how to integrate Ω quantization into a compression system with bitrate adaptation, i.e., full control over bitrate during inference. We perform experiments on both MNIST and Kodak and report on rate-distortion trade-offs comparing with the integer rounding reference. For low bitrates (&lt; 0.4 bpp), our proposed quantizer yields better performance while exhibiting also other advantageous training and inference properties. Code is available at <a href="https://github.com/ifnspaml/OHMQ" target="_blank" rel="noopener">https://github.com/ifnspaml/OHMQ</a>.
      </div>
      

      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="https://openaccess.thecvf.com/content/CVPR2022W/CLIC/papers/Lohdefink_Adaptive_Bitrate_Quantization_Scheme_Without_Codebook_for_Learned_Image_Compression_CVPRW_2022_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal"
        data-filename="/publication/loehdefink2022/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header" href="https://github.com/ifnspaml/OHMQ" target="_blank" rel="noopener">
  Code
</a>














      </div>

    </div>
    <div class="col-12 col-md-6 order-first ">
      
      
    </div>
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-6 year-2022">
          





  








  




  



<div class="col-lg-12 mb-5 view-showcase">
  <div class="row align-items-center">
    <div class="col-12 col-md-6">
      <div class="section-subheading article-title mb-0 mt-0">Joint Optimization for DNN Model Compression and Corruption Robustness</div>

      
      <div class="article-style">
        Modern deep neural networks (DNNs) are achieving state-of-the-art results due to their capability to learn a faithful representation of the data they are trained on. In this chapter, we address two insufficiencies of DNNs, namely, the lack of robustness to corruptions in the data, and the lack of real-time deployment capabilities, that need to be addressed to enable their safe and efficient deployment in real-time environments. We introduce hybrid corruption-robustness focused compression (HCRC), an approach that jointly optimizes a neural network for achieving network compression along with improvement in corruption robustness, such as noise and blurring artifacts that are commonly observed. For this study, we primarily consider the task of semantic segmentation for automated driving and focus on the interactions between robustness and compression of the network. HCRC improves the robustness of the DeepLabv3+ network by 8.39% absolute mean performance under corruption (mPC) on the Cityscapes dataset, and by 2.93% absolute mPC on the Sim KI-A dataset, while generalizing even to augmentations not seen by the network in the training process. This is achieved with only minor degradations on undisturbed data. Our approach is evaluated over two strong compression ratios (30% and 50%) and consistently outperforms all considered baseline approaches. Additionally, we perform extensive ablation studies to further leverage and extend existing state-of-the-art methods.
      </div>
      

      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="https://link.springer.com/content/pdf/10.1007/978-3-031-01233-4_15?pdf=chapter%20toc" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal"
        data-filename="/publication/varghese2022/cite.bib">
  Cite
</a>















      </div>

    </div>
    <div class="col-12 col-md-6 order-first order-md-2">
      
      
    </div>
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-6 year-2022">
          





  








  






<div class="col-lg-12 mb-5 view-showcase">
  <div class="row align-items-center">
    <div class="col-12 col-md-6">
      <div class="section-subheading article-title mb-0 mt-0">Inspect, Understand, Overcome: A Survey of Practical Methods for AI Safety</div>

      
      <div class="article-style">
        Deployment of modern data-driven machine learning methods, most often realized by deep neural networks (DNNs), in safety-critical applications such as health care, industrial plant control, or autonomous driving is highly challenging due to numerous model-inherent shortcomings. These shortcomings are diverse and range from a lack of generalization over insufficient interpretability and implausible predictions to directed attacks by means of malicious inputs. Cyber-physical systems employing DNNs are therefore likely to suffer from so-called safety concerns, properties that preclude their deployment as no argument or experimental setup can help to assess the remaining risk. In recent years, an abundance of state-of-the-art techniques aiming to address these safety concerns has emerged. This chapter provides a structured and broad overview of them. We first identify categories of insufficiencies to then describe research activities aiming at their detection, quantification, or mitigation. Our work addresses machine learning experts and safety engineers alike: The former ones might profit from the broad range of machine learning topics covered and discussions on limitations of recent methods. The latter ones might gain insights into the specifics of modern machine learning methods. We hope that this contribution fuels discussions on desiderata for machine learning systems and strategies on how to help to advance existing approaches accordingly.
      </div>
      

      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="https://link.springer.com/content/pdf/10.1007/978-3-031-01233-4_1.pdf?pdf=inline%20link" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal"
        data-filename="/publication/houben2022/cite.bib">
  Cite
</a>















      </div>

    </div>
    <div class="col-12 col-md-6 order-first ">
      
      
    </div>
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-6 year-2022">
          





  








  




  



<div class="col-lg-12 mb-5 view-showcase">
  <div class="row align-items-center">
    <div class="col-12 col-md-6">
      <div class="section-subheading article-title mb-0 mt-0">Improving Transferability of Generated Universal Adversarial Perturbations for Image Classification and Segmentation</div>

      
      <div class="article-style">
        Although deep neural networks (DNNs) are high-performance methods for various complex tasks, e.g., environment perception in automated vehicles (AVs), they are vulnerable to adversarial perturbations. Recent works have proven the existence of universal adversarial perturbations (UAPs), which, when added to most images, destroy the output of the respective perception function. Existing attack methods often show a low success rate when attacking target models which are different from the one that the attack was optimized on. To address such weak transferability, we propose a novel learning criterion by combining a low-level feature loss, addressing the similarity of feature representations in the first layer of various model architectures, with a cross-entropy loss. Experimental results on ImageNet and Cityscapes datasets show that our method effectively generates universal adversarial perturbations achieving state-of-the-art fooling rates across different models, tasks, and datasets. Due to their effectiveness, we propose the use of such novel generated UAPs in robustness evaluation of DNN-based environment perception functions for AVs.
      </div>
      

      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="https://link.springer.com/content/pdf/10.1007/978-3-031-01233-4_6.pdf?pdf=inline%20link" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal"
        data-filename="/publication/hashemi2022/cite.bib">
  Cite
</a>















      </div>

    </div>
    <div class="col-12 col-md-6 order-first order-md-2">
      
      
    </div>
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          





  








  






<div class="col-lg-12 mb-5 view-showcase">
  <div class="row align-items-center">
    <div class="col-12 col-md-6">
      <div class="section-subheading article-title mb-0 mt-0">From a Fourier-Domain Perspective on Adversarial Examples to a Wiener Filter Defense for Semantic Segmentation</div>

      
      <div class="article-style">
        Despite recent advancements, deep neural networks are not robust against adversarial perturbations. Many of the proposed adversarial defense approaches use computationally expensive training mechanisms that do not scale to complex real-world tasks such as semantic segmentation, and offer only marginal improvements. In addition, fundamental questions on the nature of adversarial perturbations and their relation to the network architecture are largely understudied. In this work, we study the adversarial problem from a frequency domain perspective. More specifically, we analyze discrete Fourier transform (DFT) spectra of several adversarial images and report two major findings: First, there exists a strong connection between a model architecture and the nature of adversarial perturbations that can be observed and addressed in the frequency domain. Second, the observed frequency patterns are largely image- and attack-type independent, which is important for the practical impact of any defense making use of such patterns. Motivated by these findings, we additionally propose an adversarial defense method based on the well-known Wiener filters that captures and suppresses adversarial frequencies in a data-driven manner. Our proposed method not only generalizes across unseen attacks but also beats five existing state-of-the-art methods across two models in a variety of attack settings.
      </div>
      

      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="https://arxiv.org/pdf/2012.01558.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal"
        data-filename="/publication/kapoor2021/cite.bib">
  Cite
</a>















      </div>

    </div>
    <div class="col-12 col-md-6 order-first ">
      
      
    </div>
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          





  








  




  



<div class="col-lg-12 mb-5 view-showcase">
  <div class="row align-items-center">
    <div class="col-12 col-md-6">
      <div class="section-subheading article-title mb-0 mt-0">Detection of Collective Anomalies in Images for Automated Driving Using an Earth Mover’s Deviation (EMDEV) Measure</div>

      
      <div class="article-style">
        For visual perception in automated driving, a reliable detection of so-called corner cases is important. Corner cases appear in many different forms and can be image frame- or sequence-related. In this work, we consider a specific type of corner case: collective anomalies. These are instances that appear in unusually large amounts in an image. We propose a detection method for collective anomalies based on a comparison of a test (sub-)set instance distribution to a training (i.e., reference) instance distribution, both distributions obtained by an instance-based semantic segmentation. For this comparison, we propose a novel so-called earth mover’s deviation (EMDEV) measure, which is able to provide signed deviations of instance distributions. Further, we propose a sliding window approach to allow the comparison of instance distributions in an online application in the vehicle. With our approach, we are able to identify collective anomalies by the proposed EMDEV measure, and to detect deviations from the instance distribution of the reference dataset.
      </div>
      

      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="https://ieeexplore.ieee.org/document/9669217" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal"
        data-filename="/publication/breitenstein2021/cite.bib">
  Cite
</a>















      </div>

    </div>
    <div class="col-12 col-md-6 order-first order-md-2">
      
      
    </div>
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          





  








  






<div class="col-lg-12 mb-5 view-showcase">
  <div class="row align-items-center">
    <div class="col-12 col-md-6">
      <div class="section-subheading article-title mb-0 mt-0">Improving Online Performance Prediction for Semantic Segmentation</div>

      
      <div class="article-style">
        In this work we address the task of observing the performance of a semantic segmentation deep neural network (DNN) during online operation, i.e., during inference, which is of high importance in safety-critical applications such as autonomous driving. Here, many high-level decisions rely on such DNNs, which are usually evaluated offline, while their performance in online operation remains unknown. To solve this problem, we propose an improved online performance prediction scheme, building on a recently proposed concept of predicting the primary semantic segmentation task&rsquo;s performance. This can be achieved by evaluating the auxiliary task of monocular depth estimation with a measurement supplied by a LiDAR sensor and a subsequent regression to the semantic segmentation performance. In particular, we propose (i) sequential training methods for both tasks in a multi-task training setup, (ii) to share the encoder as well as parts of the decoder between both task&rsquo;s networks for improved efficiency, and (iii) a temporal statistics aggregation method, which significantly reduces the performance prediction error at the cost of a small algorithmic latency. Evaluation on the KITTI dataset shows that all three aspects improve the performance prediction compared to previous approaches.
      </div>
      

      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="https://openaccess.thecvf.com/content/CVPR2021W/SAIAD/papers/Klingner_Improving_Online_Performance_Prediction_for_Semantic_Segmentation_CVPRW_2021_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal"
        data-filename="/publication/klingner2021/cite.bib">
  Cite
</a>















      </div>

    </div>
    <div class="col-12 col-md-6 order-first ">
      
      
    </div>
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          





  








  




  



<div class="col-lg-12 mb-5 view-showcase">
  <div class="row align-items-center">
    <div class="col-12 col-md-6">
      <div class="section-subheading article-title mb-0 mt-0">An Unsupervised Temporal Consistency (TC) Loss To Improve the Performance of Semantic Segmentation Networks</div>

      
      <div class="article-style">
        Deep neural networks (DNNs) for highly automated driving are often trained on a large and diverse dataset, and evaluation metrics are reported usually on a per-frame basis. However, when evaluated on video sequences, the predictions are often unstable between consecutive frames. As such unstable predictions over time can lead to severe safety consequences, there is a growing need to understand, evaluate, and improve the temporal consistency of DNNs. In this paper, we explore such a temporal characteristic and propose a novel unsupervised temporal consistency (TC) loss that penalizes unstable semantic segmentation predictions. This loss function is used in a two-stage training scheme to jointly optimize for both, accuracy of semantic segmentation predictions, and its temporal consistency based on video sequences. We demonstrate that our training strategy helps in improving the temporal consistency of two state-of-the-art semantic segmentation networks on two different road-scenes datasets. We report an absolute 4.25% improvement in the mean temporal consistency (mTC) of the HRNetV2 network and an absolute 2.78% improvement on the DeepLabv3+ network, both evaluated on the Cityscapes dataset, with only a slight decrease in accuracy. When evaluating on the same video sequences using a synthetic dataset Sim KI-A, we show absolute improvements in both, accuracy (2.19% mIoU) and temporal consistency (0.21% mTC) for the DeepLabv3+ network. We confirm similar improvements for the HRNetV2 network.
      </div>
      

      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="https://openaccess.thecvf.com/content/CVPR2021W/SAIAD/papers/Varghese_An_Unsupervised_Temporal_Consistency_TC_Loss_To_Improve_the_Performance_CVPRW_2021_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal"
        data-filename="/publication/varghese2021/cite.bib">
  Cite
</a>















      </div>

    </div>
    <div class="col-12 col-md-6 order-first order-md-2">
      
      
    </div>
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2021">
          





  








  






<div class="col-lg-12 mb-5 view-showcase">
  <div class="row align-items-center">
    <div class="col-12 col-md-6">
      <div class="section-subheading article-title mb-0 mt-0">The Vulnerability of Semantic Segmentation Networks to Adversarial Attacks in Autonomous Driving: Enhancing Extensive Environment Sensing</div>

      
      <div class="article-style">
        Enabling autonomous driving (AD) can be considered one of the biggest challenges in today&rsquo;s technology. AD is a complex task accomplished by several functionalities, with environment perception being one of its core functions. Environment perception is usually performed by combining the semantic information captured by several sensors, i.e., lidar or camera. The semantic information from the respective sensor can be extracted by using convolutional neural networks (CNNs) for dense prediction. In the past, CNNs constantly showed state-of-the-art performance on several vision-related tasks, such as semantic segmentation of traffic scenes using nothing but the red-green-blue (RGB) images provided by a camera. Although CNNs obtain state-of-the-art performance on clean images, almost imperceptible changes to the input, referred to as adversarial perturbations, may lead to fatal deception. The goal of this article is to illuminate the vulnerability aspects of CNNs used for semantic segmentation with respect to adversarial attacks, and share insights into some of the existing known adversarial defense strategies. We aim to clarify the advantages and disadvantages associated with applying CNNs for environment perception in AD to serve as a motivation for future research in this field.
      </div>
      

      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="https://arxiv.org/pdf/2101.03924.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal"
        data-filename="/publication/baer2021/cite.bib">
  Cite
</a>















      </div>

    </div>
    <div class="col-12 col-md-6 order-first ">
      
      
    </div>
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2020">
          





  








  




  



<div class="col-lg-12 mb-5 view-showcase">
  <div class="row align-items-center">
    <div class="col-12 col-md-6">
      <div class="section-subheading article-title mb-0 mt-0">Transferable Universal Adversarial Perturbations Using Generative Models</div>

      
      <div class="article-style">
        Deep neural networks tend to be vulnerable to adversarial perturbations, which by adding to a natural image can fool a respective model with high confidence. Recently, the existence of image-agnostic perturbations, also known as universal adversarial perturbations (UAPs), were discovered. However, existing UAPs still lack a sufficiently high fooling rate, when being applied to an unknown target model. In this paper, we propose a novel deep learning technique for generating more transferable UAPs. We utilize a perturbation generator and some given pretrained networks so-called source models to generate UAPs using the ImageNet dataset. Due to the similar feature representation of various model architectures in the first layer, we propose a loss formulation that focuses on the adversarial energy only in the respective first layer of the source models. This supports the transferability of our generated UAPs to any other target model. We further empirically analyze our generated UAPs and demonstrate that these perturbations generalize very well towards different target models. Surpassing the current state of the art in both, fooling rate and model-transferability, we can show the superiority of our proposed approach. Using our generated non-targeted UAPs, we obtain an average fooling rate of 93.36% on the source models (state of the art: 82.16%). Generating our UAPs on the deep ResNet-152, we obtain about a 12% absolute fooling rate advantage vs. cutting-edge methods on VGG-16 and VGG-19 target models.
      </div>
      

      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="https://arxiv.org/pdf/2010.14919.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal"
        data-filename="/publication/hashemi2020/cite.bib">
  Cite
</a>















      </div>

    </div>
    <div class="col-12 col-md-6 order-first order-md-2">
      
      
    </div>
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          





  








  






<div class="col-lg-12 mb-5 view-showcase">
  <div class="row align-items-center">
    <div class="col-12 col-md-6">
      <div class="section-subheading article-title mb-0 mt-0">Focussing Learned Image Compression to Semantic Classes for V2X Applications</div>

      
      <div class="article-style">
        Cooperative perception with many sensors involved greatly improves the performance of perceptual systems in autonomous vehicles. However, the increasing amount of sensor data leads to a bottleneck due to limited capacity of vehicle-to-X (V2X) communication channels. We leverage lossy learned image compression by means of an autoencoder with adversarial loss function to reduce the overall bitrate. Our key contribution is to focus image compression on regions of interest (ROIs) governed by a binary mask. A transmitter-sided semantic segmentation network extracts semantically important classes being the basis for the generation of a ROI. A second key contribution is that the mask is not transmitted as side information, only the quantized bottleneck data is transmitted. To train the network, we use a loss function operating only on the pixels in the ROI. We report peak-signal-to-noise ratio (PSNR) both in the entire image and only in the ROI, evaluating various fusion architectures and fusion operations involving input image and mask. Showing the high generalizability of our approach, we achieve consistent improvements in the ROI in all experiments on the Cityscapes dataset.
      </div>
      

      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="https://www.researchgate.net/profile/Jonas-Loehdefink-2/publication/346943386_Focussing_Learned_Image_Compression_to_Semantic_Classes_for_V2X_Applications/links/5fd34472299bf188d40bf392/Focussing-Learned-Image-Compression-to-Semantic-Classes-for-V2X-Applications.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal"
        data-filename="/publication/loehdefink2020/cite.bib">
  Cite
</a>















      </div>

    </div>
    <div class="col-12 col-md-6 order-first ">
      
      
    </div>
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          





  








  




  



<div class="col-lg-12 mb-5 view-showcase">
  <div class="row align-items-center">
    <div class="col-12 col-md-6">
      <div class="section-subheading article-title mb-0 mt-0">Class-Incremental Learning for Semantic Segmentation Re-Using Neither Old Data Nor Old Labels</div>

      
      <div class="article-style">
        While neural networks trained for semantic segmentation are essential for perception in autonomous driving, most current algorithms assume a fixed number of classes, presenting a major limitation when developing new autonomous driving systems with the need of additional classes. In this paper we present a technique implementing class-incremental learning for semantic segmentation without using the labeled data the model was initially trained on. Previous approaches still either rely on labels for both old and new classes, or fail to properly distinguish between them. We show how to overcome these problems with a novel class-incremental learning technique, which nonetheless requires labels only for the new classes. Specifically, (i) we introduce a new loss function that neither relies on old data nor on old labels, (ii) we show how new classes can be integrated in a modular fashion into pretrained semantic segmentation models, and finally (iii) we re-implement previous approaches in a unified setting to compare them to ours. We evaluate our method on the Cityscapes dataset, where we exceed the mIoU performance of all baselines by 3.5% absolute reaching a result, which is only 2.2% absolute below the upper performance limit of single-stage training, relying on all data and labels simultaneously.
      </div>
      

      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="https://arxiv.org/pdf/2005.06050.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal"
        data-filename="/publication/klingner2020a/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header" href="https://github.com/ifnspaml/CIL_Segmentation" target="_blank" rel="noopener">
  Code
</a>














      </div>

    </div>
    <div class="col-12 col-md-6 order-first order-md-2">
      
      
    </div>
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          





  








  






<div class="col-lg-12 mb-5 view-showcase">
  <div class="row align-items-center">
    <div class="col-12 col-md-6">
      <div class="section-subheading article-title mb-0 mt-0">Unsupervised Temporal Consistency Metric for Video Segmentation in Highly-Automated Driving</div>

      
      <div class="article-style">
        Commonly used metrics to evaluate semantic segmentation such as mean intersection over union (mIoU) do not incorporate temporal consistency. A straightforward extension of existing metrics towards evaluating the consistency of segmentation of video sequences does not exist, since labelled videos are rare and very expensive to obtain. For safety-critical applications such as highly automated driving, there is, however, a need for a metric that measures such temporal consistency of video segmentation networks to possibly support safety requirements. In this paper, (a) we introduce a metric which does not require segmentation labels for measuring the stability of the predictions of segmentation networks over a series of images; (b) we perform an in-depth analysis of the proposed metric and observe strong correlations to the supervised mIoU metric; (c) we perform an evaluation of five state-of-the-art networks for semantic segmentation of varying complexities and architectures evaluated on two public datasets, namely, Cityscapes and CamVid. Finally, we perform timing evaluations and propose the use of the metric as either an online observer for identification of possibly unstable segmentation predictions, or as an offline method to evaluate or to improve semantic segmentation networks, e.g., by selecting additional training data with critical temporal consistency.
      </div>
      

      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w20/Varghese_Unsupervised_Temporal_Consistency_Metric_for_Video_Segmentation_in_Highly-Automated_Driving_CVPRW_2020_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal"
        data-filename="/publication/varghese2020/cite.bib">
  Cite
</a>















      </div>

    </div>
    <div class="col-12 col-md-6 order-first ">
      
      
    </div>
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          





  








  




  



<div class="col-lg-12 mb-5 view-showcase">
  <div class="row align-items-center">
    <div class="col-12 col-md-6">
      <div class="section-subheading article-title mb-0 mt-0">Robust Semantic Segmentation by Redundant Networks With a Layer-Specific Loss Contribution and Majority Vote</div>

      
      <div class="article-style">
        The lack of robustness shown by deep neural networks (DNNs) questions their deployment in safety-critical tasks, such as autonomous driving. We pick up the recently introduced redundant teacher-student frameworks (3 DNNs) and propose in this work a novel error detection and correction scheme with application to semantic segmentation. It obtains its robustnesss by an online-adapted and therefore hard-to-attack student DNN during vehicle operation, which builds upon a novel layer-dependent inverse feature matching (IFM) loss. We conduct experiments on the Cityscapes dataset showing that this loss renders the adaptive student to be more than 20% absolute mean intersection-over-union (mIoU) better than in previous works. Moreover, the entire error correction virtually always delivers the performance of the best non-attacked network, resulting in an mIoU of about 50% even under strongest attacks (instead of 1&hellip;2%), while keeping the performance on clean data at about original level (ca. 75.7%).
      </div>
      

      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w20/Bar_Robust_Semantic_Segmentation_by_Redundant_Networks_With_a_Layer-Specific_Loss_CVPRW_2020_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal"
        data-filename="/publication/baer2020/cite.bib">
  Cite
</a>















      </div>

    </div>
    <div class="col-12 col-md-6 order-first order-md-2">
      
      
    </div>
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          





  








  






<div class="col-lg-12 mb-5 view-showcase">
  <div class="row align-items-center">
    <div class="col-12 col-md-6">
      <div class="section-subheading article-title mb-0 mt-0">Improved Noise and Attack Robustness for Semantic Segmentation by Using Multi-Task Training with Self-Supervised Depth Estimation</div>

      
      <div class="article-style">
        While current approaches for neural network training often aim at improving performance, less focus is put on training methods aiming at robustness towards varying noise conditions or directed attacks by adversarial examples. In this paper, we propose to improve robustness by a multi-task training, which extends supervised semantic segmentation by a self-supervised monocular depth estimation on unlabeled videos. This additional task is only performed during training to improve the semantic segmentation model&rsquo;s robustness at test time under several input perturbations. Moreover, we even find that our joint training approach also improves the performance of the model on the original (supervised) semantic segmentation task. Our evaluation exhibits a particular novelty in that it allows to mutually compare the effect of input noises and adversarial attacks on the robustness of the semantic segmentation. We show the effectiveness of our method on the Cityscapes dataset, where our multi-task training approach consistently outperforms the single-task semantic segmentation baseline in terms of both robustness vs. noise and in terms of adversarial attacks, without the need for depth labels in training.
      </div>
      

      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w20/Klingner_Improved_Noise_and_Attack_Robustness_for_Semantic_Segmentation_by_Using_CVPRW_2020_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal"
        data-filename="/publication/klingner2020/cite.bib">
  Cite
</a>















      </div>

    </div>
    <div class="col-12 col-md-6 order-first ">
      
      
    </div>
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2019">
          





  








  




  



<div class="col-lg-12 mb-5 view-showcase">
  <div class="row align-items-center">
    <div class="col-12 col-md-6">
      <div class="section-subheading article-title mb-0 mt-0">On the Robustness of Redundant Teacher-Student Frameworks for Semantic Segmentation</div>

      
      <div class="article-style">
        The trend towards autonomous systems in today’s technology comes with the need for environment perception. Deep neural networks (DNNs) constantly showed state-ofthe-art performance over the last few years in visual machine perception, e.g., semantic segmentation. While DNNs work fine on uncorrupted data, recently introduced adversarial examples (AEs) led to misclassification with high confidence. This lack of robustness against such adversarial attacks questions the use of DNNs in safety-critical autonomous systems, e.g., autonomous driving vehicles. In this work, we address the mentioned problem with the use of a redundant teacher-student framework, consisting of a static teacher network (T), a static student network (S), and a constantly adapting student network (A). By using this triplet in combination with a novel inverse feature matching (IFM) loss, we show that a significant robustness increase of student DNNs against adversarial attacks is achieveable, while maintaining semantic segmentation quality at a reasonably high level. With our approach, we manage to increase the mean intersection over union (mean IoU) ratio between static student adversarial examples and clean images from about 35 % to about 80 % on the Cityscapes dataset. Moreover, our proposed method can be integrated into any DNN-based perception mechanism to increase the (online) robustness in an adversarial environment, created from static model knowledge.
      </div>
      

      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/SAIAD/Bar_On_the_Robustness_of_Redundant_Teacher-Student_Frameworks_for_Semantic_Segmentation_CVPRW_2019_paper.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal"
        data-filename="/publication/baer2019/cite.bib">
  Cite
</a>















      </div>

    </div>
    <div class="col-12 col-md-6 order-first order-md-2">
      
      
    </div>
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2019">
          





  








  






<div class="col-lg-12 mb-5 view-showcase">
  <div class="row align-items-center">
    <div class="col-12 col-md-6">
      <div class="section-subheading article-title mb-0 mt-0">Towards Corner Case Detection for Autonomous Driving</div>

      
      <div class="article-style">
        The progress in autonomous driving is also due to the increased availability of vast amounts of training data for the underlying machine learning approaches. Machine learning systems are generally known to lack robustness, e.g., if the training data did rarely or not at all cover critical situations. The challenging task of corner case detection in video, which is also somehow related to unusual event or anomaly detection, aims at detecting these unusual situations, which could become critical, and to communicate this to the autonomous driving system (online use case). Such a system, however, could be also used in offline mode to screen vast amounts of data and select only the relevant situations for storing and (re)training machine learning algorithms. So far, the approaches for corner case detection have been limited to videos recorded from a fixed camera, mostly for security surveillance. In this paper, we provide a formal definition of a corner case and propose a system framework for both the online and the offline use case that can handle video signals from front cameras of a naturally moving vehicle and can output a corner case score.
      </div>
      

      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="https://arxiv.org/pdf/1902.09184.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal"
        data-filename="/publication/bolte2019/cite.bib">
  Cite
</a>















      </div>

    </div>
    <div class="col-12 col-md-6 order-first ">
      
      
    </div>
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2019">
          





  








  




  



<div class="col-lg-12 mb-5 view-showcase">
  <div class="row align-items-center">
    <div class="col-12 col-md-6">
      <div class="section-subheading article-title mb-0 mt-0">On Low-Bitrate Image Compression for Distributed Automotive Perception: Higher Peak SNR Does Not Mean Better Semantic Segmentation</div>

      
      <div class="article-style">
        The high amount of sensors required for autonomous driving poses enormous challenges on the capacity of automotive bus systems. There is a need to understand tradeoffs between bitrate and perception performance. In this paper, we compare the image compression standards JPEG, JPEG2000, and WebP to a modern encoder/decoder image compression approach based on generative adversarial networks (GANs). We evaluate both the pure compression performance using typical metrics such as peak signal-to-noise ratio (PSNR), structural similarity (SSIM) and others, but also the performance of a subsequent perception function, namely a semantic segmentation (characterized by the mean intersection over union (mIoU) measure). Not surprisingly, for all investigated compression methods, a higher bitrate means better results in all investigated quality metrics. Interestingly, however, we show that the semantic segmentation mIoU of the GAN autoencoder in the highly relevant low-bitrate regime (at 0.0625 bit/pixel) is better by 3.9 % absolute than JPEG2000, although the latter still is considerably better in terms of PSNR (5.91dB difference). This effect can greatly be enlarged by training the semantic segmentation model with images originating from the decoder, so that the mIoU using the segmentation model trained by GAN reconstructions exceeds the use of the model trained with original images by almost 20 % absolute. We conclude that distributed perception in future autonomous driving will most probably not provide a solution to the automotive bus capacity bottleneck by using standard compression schemes such as JPEG2000, but requires modern coding approaches, with the GAN encoder/decoder method being a promising candidate.
      </div>
      

      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="https://arxiv.org/pdf/1902.04311.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal"
        data-filename="/publication/loehdefink2019/cite.bib">
  Cite
</a>















      </div>

    </div>
    <div class="col-12 col-md-6 order-first order-md-2">
      
      
    </div>
  </div>
</div>

        </div>

        
      </div>

    </div>
  </div>
</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  



  

  

  

  
  






  
  <p class="powered-by copyright-license-text">
    © 2023 Andreas Bär
  </p>
  




</footer>

    </div>
    
  </div>

      

    
    <script src="../js/vendor-bundle.min.46271ef31da3f018e9cd1b59300aa265.js"></script>

    
    
    
      
        <script src="https://cdn.jsdelivr.net/gh/desandro/imagesloaded@v4.1.4/imagesloaded.pkgd.min.js" integrity="sha512-S5PZ9GxJZO16tT9r3WJp/Safn31eu8uWrzglMahDT4dsmgqWonRY9grk3j+3tfuPr9WJNsfooOR7Gi7HL5W2jw==" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/gh/metafizzy/isotope@v3.0.6/dist/isotope.pkgd.min.js" integrity="sha512-Zq2BOxyhvnRFXu0+WE6ojpZLOU2jdnqbrM1hmVdGzyeCa1DgM3X5Q4A/Is9xA1IkbUeDd7755dNNI/PzSf2Pew==" crossorigin="anonymous"></script>
      

      
      

      

      
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/highlight.min.js" integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin="anonymous"></script>
        
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/r.min.js" crossorigin="anonymous"></script>
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/latex.min.js" crossorigin="anonymous"></script>
        
      

    

    
    
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    
      
      
      
      
      
      
      
    

    

    
    
    
    <script id="page-data" type="application/json">{"use_headroom":true}</script>

    
    
      <script src="../js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js" type="module"></script>
    
    
    
    
    
    
    
      
      
    
    
    <script src="../en/js/wowchemy.min.3239609ca7abe02ed52d76ffcb22af1d.js"></script>

    
    
    
    
    
    
      
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      <script src="../js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js" type="module"></script>






</body>
</html>
